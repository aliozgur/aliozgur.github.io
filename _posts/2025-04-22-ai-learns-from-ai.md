---
layout: post
title: "When AI Learns From AI"
subtitle: "The Risk of Losing Human Knowledge in a Synthetic Feedback Loop"
date: 2025-04-22
author: "Ali Özgür"
excerpt_separator: <!--end-of-excerpt-->
published: true
tags:
  - AI
  - Learning
  - Philosophy
  - Technology
---

AI is no longer a futuristic concept - it's embedded in how we work, write, code, communicate, and even think. I've been reflecting on a question that feels more urgent by the day:

**What if AI usage, as it becomes more widespread, slowly erodes the very foundations of human learning and creativity?**

![Cover]({{ root.url }}/media/ai-learns-from-ai.png)

<!--end-of-excerpt-->

## From Tool to Crutch

When calculators became common in classrooms, people asked: *Will this stop kids from learning math?*  
We know now that calculators didn't kill math — but they did shift *how* it's taught.

With AI, the shift is exponentially more drastic. It's not just solving equations. It's writing essays, generating code, composing music, summarizing articles, creating marketing plans.

It's thinking on our behalf.

And here lies the dilemma: if we offload enough of our thinking to machines, will we stop practicing the cognitive muscles that make us human?

### Example 1: Code Autocomplete and Junior Devs

Take GitHub Copilot. It's amazing — I use it, and I love the speed boost.  
But I've noticed something interesting with junior developers: they're less likely to read the docs, debug through the stack, or even ask questions in Slack.

Why? Because Copilot “just works” — until it doesn't. And when it breaks, the understanding gap is exposed. Without the foundation of why the code works, they're often stuck. It's like driving a self-driving car without knowing how to take control of the wheel when something goes wrong.

## How AI Is Trained?

AI's current capabilities are impressive — but not magical. These systems are statistical engines trained on enormous amounts of human-generated data. 

Most modern language models, like GPT and LLaMA, are trained on a combination of:

- Books and academic papers  
- Wikipedia  
- Public code repositories (like GitHub)  
- Social media discussions  
- Online forums and blogs  
- News articles and documentation

The underlying architecture used is the Transformer, introduced in the ["Attention is All You Need"](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need) paper by Vaswani et al. in 2017.

LLMs typically go through two key stages:

1. Pretraining — where the model passively absorbs a large amount of general text.
2. Fine-tuning — where it is refined for specific tasks, sometimes with human feedback (as in Reinforcement Learning with Human Feedback, or RLHF).

If you'd like to read more, IBM offers a solid overview:  
[What Are Large Language Models (LLMs)? – IBM](https://www.ibm.com/think/topics/large-language-models)

## The Decay of Original Thought

Let's take the thought further.

If more and more content is generated by AI, and future models are trained on this content, we risk entering a synthetic feedback loop — a world where AI trains on AI output.

### Example 2: AI-Written Docs Training New AIs

Imagine a scenario where technical documentation is mostly written by AI. It reads well, it's grammatically perfect — but it lacks nuance. It avoids edge cases. It parrots what is “commonly correct,” not what is “contextually right.”

Now imagine training the next generation of models on this sanitized, AI-authored corpus.

Each generation becomes more detached from human context. More syntactic, less semantic. More correct-looking, but less useful in real-world complexity.

This is not just speculative. Research already warns about a dangerous trend called model collapse.

## Model Collapse: When AI Eats AI

A recent [study published in *Nature*](https://www.nature.com/articles/s41586-024-07566-y) explores the risk of training LLMs on data that was itself generated by other LLMs. The result?

> “Performance collapses rapidly. The models lose their ability to generalize or represent reality as they spiral into self-referential noise.”

The phenomenon is called model collapse, and it's akin to digital inbreeding:  
The model becomes a photocopy of a photocopy of a photocopy — slowly degrading into useless output.

For a good summary of the implications, also see:  
[Model Collapse Threatens to Kill Progress on Generative AIs – Freethink](https://www.freethink.com/robots-ai/model-collapse-synthetic-data)

## Knowledge as Fossil Fuel

Here's a metaphor I keep coming back to:

AI models are powered by the fossil fuel of human knowledge — a resource created through effort, time, mistakes, and experience.

If we stop producing that fuel — because we let AI do all the creating — the fuel reserves dry up.

No more open-source experiments. No more weird side projects. No more personal blog rants.  
Just polished, engineered, SEO-optimized synthetic content — forever eating its own tail.

## The Illusion of Learning

AI usage can feel like learning. When ChatGPT explains a topic to you, it's seductive. You feel smart.  
But that feeling doesn't always translate to understanding.

You didn't build the mental model — you just received the output.

And over time, if this becomes your default mode of learning, you might lose the ability to wrestle with complexity, to sit in ambiguity, to follow a train of thought beyond a 3-sentence summary.

### Example 3: Students Who Don't Write Anymore

In university classrooms today, professors are struggling with essays written entirely by ChatGPT.  
They read fluently. The citations are plausible. But they lack soul. No friction. No exploration.  
Just… completion.

Worse: students say, “But I learned from the output.”  
No. What you learned was how to get an answer, not how to form a question.

## The Real Risk: Losing the Source

Let's be brutally honest.

If everyone starts consuming AI-generated answers, and no one creates new, messy, original content from scratch, we lose the source material that powers progress — not just for machines, but for ourselves.

And then?

The next generation of AI becomes like a photocopy of a photocopy. Sharper edges lost. Context faded. Errors amplified.  
A simulation of knowledge without depth.

## Toward Creative Resistance

So what's the antidote?

Here's what I'm trying to practice:

- Use AI, but don't trust it blindly. Let it spark ideas, not replace thinking.  
- Write original content, even if imperfect. The internet needs your voice, not another echo.  
- Struggle on purpose. That bug you fixed after 3 hours of debugging? That's real learning.  
- Go deeper than summaries. Read full papers, full books. Not just TL;DRs.  
- Mentor, teach, converse. Shared human thinking is generative. Models just remix it.

We need AI. But AI also needs us — as creators, not just consumers.

Let's make sure humanity doesn't outsource its mind.

---

What do you think? Have you noticed changes in your own thinking or learning since using AI?  
Let's talk.


***
{% include share_twitter_tr.html %}

***
